# -*- coding: utf-8 -*-
"""Sistem Rekomendasi IoT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ARW8iWrnWgM3m39LcGcrZAFsbCUaIck

# **SISTEM REKOMENDASI ARTIKEL ILMIAH**

Project ini dibuat untuk memberikan kemudahan dalam menentukan artikel ilmiah yang akan dibaca lebih lanjut.

**Langkah 1: Memuat Dataset**

Kita akan mengunduh dan memuat data dari GitHub. Kita akan membaca file CSV menggunakan pandas dan memeriksa struktur datanya.
"""

import pandas as pd

# URL file CSV di GitHub (ganti dengan URL file Anda)
url = "https://raw.githubusercontent.com/hollandakusuma/recommendationSystemIoT/refs/heads/main/Artikel%20IoT.xlsx"

# Membaca data CSV
data = pd.read_excel(url)

# Menampilkan data 5 baris pertama
print(data.head())

"""# **DATA UNDERSTANDING**
**Langkah 2: Eksplorasi Data**

Setelah berhasil memuat file, lakukan langkah eksplorasi awal yang sama seperti pada data CSV:


1.   Tampilkan kolom yang tersedia.
2.   Identifikasi kolom yang relevan untuk analisis.
"""

# 1. Menampilkan informasi dataset
print("INFORMASI DATASET:")
print(data.info())

# 2. Cek nilai kosong
print("\nCEK NILAI KOSONG:")
print(data.isnull().sum())

# Menampilkan kolom yang tersedia
print("\nKOLOM DATA YANG TERSEDIA:")
print(data.columns)

# 3. Deskripsi Statistik
print("\nDeskripsi Statistik Kolom Numerik:")
print(data.describe())

# 4. Distribusi Data Unik
print("\nDistribusi Data Unik di Kolom 'Source':")
print(data['Source'].value_counts())

"""#**Univariate Exploratory Data Analysis (EDA)**
**2. Distribusi dan Frekuensi Data**

*    **Distribusi Jumlah Sitasi**: Memahami artikel mana yang paling banyak dikutip.
*    **Frekuensi Tipe Artikel (Type)**: Mengetahui sebaran jenis artikel.
*    **Frekuensi Source** : Mengetahui sebaran tempat Artikel Ilmiah dipublikasikan
*    **Penulis dengan artikel terbanyak**: Mengidentifikasi penulis yang memiliki artikel terbanyak dalam dataset.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Distribusi Jumlah Sitasi
plt.figure(figsize=(10, 6))
sns.histplot(data['Cites'], kde=True, bins=50, color='blue')
plt.title('Distribusi Jumlah Sitasi')
plt.xlabel('Jumlah Sitasi')
plt.ylabel('Frekuensi')

# Menambahkan Xticks dengan interval tertentu, misalnya setiap 100 sitasi
plt.xticks(ticks=range(0, int(data['Cites'].max())+100, 100))
plt.show()

# 2. Frekuensi Tipe Artikel
plt.figure(figsize=(10, 6))
sns.countplot(data['Type'], color='blue')
plt.title('Frekuensi Tipe Artikel')
plt.xlabel('Tipe Artikel')
plt.ylabel('Frekuensi')
plt.xticks(rotation=45)
plt.show()

# 3. Frekuensi Sumber Artikel
source_counts = data['Source'].value_counts()
source_counts_filtered = source_counts[source_counts > 5]

plt.figure(figsize=(12, 12))
sns.countplot(y=data['Source'], color='blue', order=source_counts_filtered.index)
plt.title('Frekuensi Sumber Artikel')
plt.xlabel('Frekuensi')
plt.ylabel('Source')
plt.show()

# 4. Penulis dengan Artikel Terbanyak
# Menghitung jumlah artikel per penulis
authors_count = data['Authors'].value_counts().head(10)  # 10 penulis terbanyak
plt.figure(figsize=(10, 4))
authors_count.plot(kind='bar', color='green')
plt.title('Penulis dengan Artikel Terbanyak')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Artikel')
plt.xticks(rotation=45)
plt.show()

"""#**DATA PREPARATION**
**1. Penghapusan Kolom Tidak Relevan**

Pada dataset asli, terdapat beberapa kolom yang kosong atau tidak relevan dengan tujuan proyek. Kolom-kolom ini dihapus untuk mengurangi kompleksitas data dan meningkatkan efisiensi pemrosesan. Berikut adalah kolom yang dihapus:

- Publisher: Kolom ini sepenuhnya kosong dan tidak mengandung informasi yang berguna untuk analisis atau rekomendasi artikel.

- ECC: Kolom ini berisi informasi yang tidak relevan dengan pengembangan sistem rekomendasi, karena tidak mempengaruhi kesamaan artikel.

- Abstract: Tidak memiliki data yang terisi, dan meskipun dapat menjadi sumber informasi penting dalam beberapa model, kolom ini tidak digunakan dalam model rekomendasi yang dirancang.

- FullTextURL: Kolom ini tidak memberikan kontribusi langsung terhadap pengolahan data untuk fitur rekomendasi.

- RelatedURL: Kolom ini juga kosong dan tidak relevan dengan fitur atau tujuan sistem rekomendasi.
Volume, Issue, StartPage, dan EndPage: Kolom-kolom ini berisi detail teknis tentang publikasi yang tidak memiliki kontribusi langsung terhadap penentuan relevansi artikel dalam konteks sistem rekomendasi.

- CitesPerYear: Meskipun berguna dalam konteks analisis sitasi, kolom ini tidak digunakan dalam model rekomendasi yang lebih berfokus pada jumlah sitasi total (Cites).

- GSRank, QueryDate, CitesURL, ISSN, CitationURL, dan Age: Kolom-kolom ini juga tidak memberikan informasi yang relevan atau langsung mempengaruhi hasil rekomendasi yang berbasis pada judul artikel dan jumlah sitasi.
"""

# Hapus kolom yang kosong atau tidak relevan
columns_to_drop = ['Publisher', 'ECC','Abstract', 'FullTextURL', 'RelatedURL',
                   'Volume', 'Issue', 'StartPage', 'EndPage',
                   'GSRank','QueryDate','CitesURL','ISSN','CitationURL','Age']

# Buang kolom dari dataset
data_cleaned = data.drop(columns=columns_to_drop)

# Tampilkan informasi dataset setelah pembersihan
print(data_cleaned.info())

# Tampilkan data beberapa baris pertama
print(data_cleaned.head())

"""# **MODEL DEVELOPMENT : CONTENT BASED FILTERING**

1. Preprocessing Teks

Pembersihan Teks: Menormalisasi teks pada kolom Title (menghapus karakter yang tidak diinginkan, mengubah teks ke huruf kecil, menghapus stopwords, dsb.).
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import string
from nltk.corpus import stopwords

# Mengunduh stopwords dari NLTK
import nltk
nltk.download('stopwords')

# Fungsi untuk membersihkan teks
def clean_text(text):
    text = text.lower()  # Mengubah teks menjadi huruf kecil
    text = ''.join([char for char in text if char not in string.punctuation])  # Menghapus tanda baca
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Menghapus stopwords
    return text

# Terapkan pembersihan teks pada kolom 'Title'
data_cleaned['Title_cleaned'] = data_cleaned['Title'].apply(clean_text)

# Menampilkan beberapa judul setelah pembersihan
print(data_cleaned[['Title', 'Title_cleaned']].head())

"""2. Representasi Teks

Menggunakan TF-IDF (Term Frequency-Inverse Document Frequency) untuk mengonversi teks judul artikel menjadi vektor numerik yang dapat diproses oleh algoritma.
"""

# Menggunakan TF-IDF Vectorizer untuk mengonversi teks menjadi representasi numerik
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(data_cleaned['Title_cleaned'])

# Menampilkan dimensi matriks TF-IDF
print(f'TF-IDF Matrix Shape: {tfidf_matrix.shape}')

"""3. Menghitung Kemiripan Antar Artikel

Menggunakan Cosine Similarity untuk menghitung kemiripan antara artikel yang diminta dengan artikel lainnya berdasarkan vektor TF-IDF mereka.
"""

# Menghitung kemiripan antar artikel menggunakan Cosine Similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Menampilkan matriks kemiripan untuk artikel pertama
print(cosine_sim[0])

"""4. Merekomendasikan Artikel

Menyaring 10 artikel paling relevan yang memiliki kemiripan tinggi dengan artikel yang diminta, serta mempertimbangkan jumlah sitasi.

Langkah-langkah:

1.    Mengambil Input Keyword dari Pengguna: Menggunakan input keyword untuk mencari artikel yang relevan berdasarkan judul.
2.    Mencocokkan Keyword dengan Artikel: Memfilter artikel yang mengandung kata kunci dari input pengguna.
3.    Menghitung Kemiripan dengan Cosine Similarity: Menggunakan cosine similarity berdasarkan judul artikel yang mengandung keyword.
4.    Menyajikan Hasil dalam Bentuk Tabel: Menampilkan hasil rekomendasi dalam bentuk tabel yang berisi Artikel ID, Title, Cites, Type, dan DOI.
"""

import pandas as pd
from google.colab import data_table
from IPython.display import display

def recommend_articles_by_keyword(keyword, top_n=10):
    # Menyaring artikel yang mengandung keyword pada judul
    filtered_data = data_cleaned[data_cleaned['Title'].str.contains(keyword, case=False, na=False)]

    if filtered_data.empty:
        return "Tidak ada artikel yang mengandung kata kunci tersebut."

    # Preprocessing dan TF-IDF Vectorization pada filtered data
    tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_data['Title_cleaned'])

    # Menghitung cosine similarity antar artikel
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

    # Menghitung bobot berdasarkan jumlah sitasi
    cites_weight = filtered_data['Cites'] / filtered_data['Cites'].max()
    weighted_cosine_sim = cosine_sim * cites_weight.values.reshape(-1, 1)

    # Menyaring artikel paling relevan berdasarkan cosine similarity
    sim_scores = list(enumerate(weighted_cosine_sim[0]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # Menghindari artikel yang sama dengan input

    # Menyusun daftar rekomendasi
    recommended_idx = [i[0] for i in sim_scores]
    recommended_articles_content = filtered_data.iloc[recommended_idx][['Authors','Title', 'Cites', 'Type', 'DOI']]

    # Menambahkan Artikel ID (indeks artikel)
    recommended_articles_content['Artikel ID'] = recommended_articles_content.index
    recommended_articles_content = recommended_articles_content[['Artikel ID', 'Authors','Title', 'Cites', 'Type', 'DOI']]

    return recommended_articles_content

# Fungsi untuk menampilkan hasil dalam bentuk tabel interaktif
def display_recommended_articles_table(keyword, top_n=10):
    # Mendapatkan rekomendasi artikel berdasarkan keyword
    recommended_articles_content = recommend_articles_by_keyword(keyword, top_n)

    if isinstance(recommended_articles, str):
        # Jika tidak ada artikel yang ditemukan, print pesan
        print(recommended_articles_content)
    else:
        # Menampilkan rekomendasi dalam format tabel interaktif menggunakan data_table
        display(data_table.DataTable(recommended_articles_content))  # Menggunakan display untuk menampilkan tabel interaktif

# Menerima input keyword dari pengguna
user_keyword = input("Masukkan keyword untuk mencari artikel: ")
recommended_articles_content = recommend_articles_by_keyword(user_keyword, top_n=10)
display_recommended_articles_table(user_keyword, top_n=10)

"""5. Matriks Evaluasi **Precision at K (P@K)**

Precision mengukur sejauh mana artikel yang direkomendasikan relevan dengan preferensi pengguna. Precision@K mengukur jumlah artikel relevan dalam K rekomendasi teratas. Dalam konteks ini, kita bisa menganggap artikel yang memiliki sitasi tinggi sebagai relevan.

Rumusnya:
$${P@K} = \frac{A​}{K}
$$

Di mana:
-   $A$ adalah Jumlah artikel relevan dalam K rekomendasi.
- $K$ adalah Jumlah rekomendasi.


"""

# Evaluasi Precision@K dan Recall@K
def evaluate_recommendations(recommended_articles, relevant_articles, k=10):
    """
    recommended_articles: DataFrame hasil rekomendasi
    relevant_articles: DataFrame yang berisi artikel relevan yang sesungguhnya
    k: jumlah rekomendasi teratas yang akan dievaluasi
    """

    # Ambil K artikel teratas
    top_k_recommendations = recommended_articles.head(k)

    # Ambil artikel relevan yang ada dalam daftar artikel yang relevan
    relevant_articles_set = set(relevant_articles['Title'])

    # Hitung Precision@K
    relevant_in_top_k = top_k_recommendations[top_k_recommendations['Title'].isin(relevant_articles_set)]
    precision_at_k = len(relevant_in_top_k) / k


    return precision_at_k

# Contoh Penggunaan:
# Misalkan 'relevant_articles' adalah daftar artikel yang relevan berdasarkan sitasi
relevant_articles = data_cleaned[data_cleaned['Cites'] > 10]  # Artikel dengan sitasi > 10 dianggap relevan

# Melakukan evaluasi untuk artikel yang direkomendasikan
precision_at_k= evaluate_recommendations(recommended_articles_content, relevant_articles, k=10)

# Menampilkan hasil evaluasi
print(f"Precision@10: {precision_at_k:.4f}")

"""# **MODEL DEVELOPMENT HYBRID METHODS**

1. **Content-Based Filtering**:
   - Fungsi `recommend_articles_by_keyword()` digunakan untuk mencari artikel yang relevan dengan kata kunci tertentu dalam judul. Artikel yang mengandung kata kunci tersebut akan dipilih dan diolah menggunakan **TF-IDF Vectorizer** untuk mengubah teks judul artikel menjadi representasi numerik.
   - Kemudian, dihitung **cosine similarity** antar artikel untuk menemukan artikel yang paling mirip dengan artikel yang dicari. Artikel-article dengan tingkat kemiripan tertinggi akan direkomendasikan.

2. **Item-Based Collaborative Filtering**:
   - Fungsi `item_based_collaborative_filtering()` mengandalkan fitur-fitur seperti jumlah sitasi (`Cites`) dan jumlah sitasi per tahun (`CitesPerYear`) untuk membangun model **K-Nearest Neighbors (KNN)**.
   - Artikel-artikel yang memiliki kesamaan dalam fitur-fitur ini akan dikelompokkan dan diberi skor kemiripan menggunakan jarak **cosine similarity**.

3. **Sistem Hybrid**:
   - Fungsi `recommend_articles()` menggabungkan kedua pendekatan tersebut. Pertama, artikel direkomendasikan berdasarkan **Content-Based Filtering** menggunakan kata kunci dari pengguna.
   - Jika artikel yang relevan ditemukan, fungsi kemudian melanjutkan untuk melakukan **Collaborative Filtering** pada artikel-artikel tersebut, menggunakan KNN untuk menemukan artikel serupa berdasarkan fitur seperti jumlah sitasi dan jenis artikel.
   - Artikel yang direkomendasikan akan disusun berdasarkan tingkat kesamaan dan ditampilkan dalam bentuk tabel.

4. **Output**:
   - Program ini menampilkan artikel-artikel yang paling relevan sesuai dengan kata kunci yang diberikan oleh pengguna, serta memberikan informasi tambahan seperti judul, penulis, jumlah sitasi, dan DOI artikel.



"""

from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Fungsi untuk rekomendasi berdasarkan Content-Based Filtering
def recommend_articles_by_keyword(keyword, data, top_n=10):
    # Menyaring artikel yang mengandung keyword pada judul
    filtered_data = data[data['Title'].str.contains(keyword, case=False, na=False)]

    if filtered_data.empty:
        return "Tidak ada artikel yang mengandung kata kunci tersebut."

    # Preprocessing dan TF-IDF Vectorization pada filtered data
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_data['Title'])

    # Menghitung cosine similarity antar artikel
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

    # Menyaring artikel paling relevan berdasarkan cosine similarity
    sim_scores = list(enumerate(cosine_sim[0]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # Menghindari artikel yang sama dengan input

    # Menyusun daftar rekomendasi
    recommended_idx = [i[0] for i in sim_scores]
    recommended_articles = filtered_data.iloc[recommended_idx][['Authors','Title', 'Cites', 'Type', 'DOI']]

    return recommended_articles

# Fungsi untuk Item-Based Collaborative Filtering
def item_based_collaborative_filtering(data, top_n=10):
    # Pilih fitur yang digunakan untuk Collaborative Filtering
    item_features = data[['Cites', 'CitesPerYear']].copy()  # Menggunakan 'Cites' dan 'CitesPerYear'

    # Mengonversi 'Type' menjadi variabel dummy (one-hot encoding)
    type_dummies = pd.get_dummies(data['Type'], prefix='Type')
    item_features = pd.concat([item_features, type_dummies], axis=1)

    # Normalisasi data menggunakan StandardScaler
    scaler = StandardScaler()
    normalized_features = scaler.fit_transform(item_features)

    # Membangun model KNN untuk menemukan kesamaan antar artikel
    knn = NearestNeighbors(n_neighbors=top_n, metric='cosine')
    knn.fit(normalized_features)

    # Menghitung kesamaan artikel berdasarkan fitur yang telah dinormalisasi
    distances, indices = knn.kneighbors(normalized_features)

    return distances, indices

# Fungsi untuk mendapatkan rekomendasi berdasarkan Content-Based dan Collaborative Filtering
def recommend_articles(keyword, data, top_n=10):
    # Dapatkan rekomendasi berdasarkan keyword (Content-Based Filtering)
    content_based_recommendations = recommend_articles_by_keyword(keyword, data, top_n)

    if isinstance(content_based_recommendations, str):
        # Jika tidak ada artikel berdasarkan keyword
        return content_based_recommendations

    # Jika ada artikel berdasarkan keyword, lanjutkan ke Collaborative Filtering
    recommended_articles = content_based_recommendations

    # Mengambil index dari artikel yang relevan berdasarkan keyword
    relevant_idx = recommended_articles.index.tolist()

    # Ambil data artikel yang relevan dari data_cleaned
    relevant_data = data.iloc[relevant_idx]

    # Membangun model Collaborative Filtering berdasarkan artikel relevan
    distances, indices = item_based_collaborative_filtering(relevant_data, top_n)

    # Menyusun rekomendasi artikel dari hasil Collaborative Filtering
    recommended_articles_collaborative = relevant_data.iloc[indices[0]].copy()
    recommended_articles_collaborative['Similarity'] = 1 - distances[0]

    # Menyusun hasil rekomendasi
    recommended_articles_collaborative = recommended_articles_collaborative[['Artikel ID', 'Authors', 'Title', 'Cites', 'Type', 'DOI', 'Similarity']]
    recommended_articles_collaborative = recommended_articles_collaborative.sort_values(by='Similarity', ascending=False)

    return recommended_articles_collaborative

# Contoh penggunaan sistem rekomendasifrom sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Fungsi untuk rekomendasi berdasarkan Content-Based Filtering
def recommend_articles_by_keyword(keyword, data, top_n=10):
    # Menyaring artikel yang mengandung keyword pada judul
    filtered_data = data[data['Title'].str.contains(keyword, case=False, na=False)]

    if filtered_data.empty:
        return "Tidak ada artikel yang mengandung kata kunci tersebut."

    # Preprocessing dan TF-IDF Vectorization pada filtered data
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_data['Title'])

    # Menghitung cosine similarity antar artikel
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

    # Menyaring artikel paling relevan berdasarkan cosine similarity
    sim_scores = list(enumerate(cosine_sim[0]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # Menghindari artikel yang sama dengan input

    # Menyusun daftar rekomendasi
    recommended_idx = [i[0] for i in sim_scores]
    recommended_articles = filtered_data.iloc[recommended_idx][['Authors','Title', 'Cites', 'Type', 'DOI']]

    # Menambahkan Artikel ID (indeks artikel)
    recommended_articles['Artikel ID'] = recommended_articles.index
    recommended_articles = recommended_articles[['Artikel ID', 'Authors', 'Title', 'Cites', 'Type', 'DOI']]

    return recommended_articles

# Fungsi untuk Item-Based Collaborative Filtering
def item_based_collaborative_filtering(data, top_n=10):
    # Pilih fitur yang digunakan untuk Collaborative Filtering
    item_features = data[['Cites', 'CitesPerYear']].copy()  # Menggunakan 'Cites' dan 'CitesPerYear'

    # Mengonversi 'Type' menjadi variabel dummy (one-hot encoding)
    type_dummies = pd.get_dummies(data['Type'], prefix='Type')
    item_features = pd.concat([item_features, type_dummies], axis=1)

    # Normalisasi data menggunakan StandardScaler
    scaler = StandardScaler()
    normalized_features = scaler.fit_transform(item_features)

    # Membangun model KNN untuk menemukan kesamaan antar artikel
    knn = NearestNeighbors(n_neighbors=top_n, metric='cosine')
    knn.fit(normalized_features)

    # Menghitung kesamaan artikel berdasarkan fitur yang telah dinormalisasi
    distances, indices = knn.kneighbors(normalized_features)

    return distances, indices

def recommend_articles(keyword, data, top_n=10):
    # Dapatkan rekomendasi berdasarkan keyword (Content-Based Filtering)
    content_based_recommendations = recommend_articles_by_keyword(keyword, data, top_n)

    if isinstance(content_based_recommendations, str):
        return content_based_recommendations

    # Jika ada artikel berdasarkan keyword, lanjutkan ke Collaborative Filtering
    recommended_articles = content_based_recommendations

    # Mengambil index dari artikel yang relevan berdasarkan keyword
    relevant_idx = recommended_articles.index.tolist()

    # Ambil data artikel yang relevan dari data_cleaned
    relevant_data = data.iloc[relevant_idx]

    # Tentukan top_n berdasarkan jumlah artikel relevan yang ada
    top_n = min(len(relevant_data), top_n)  # Ambil yang lebih kecil antara jumlah artikel relevan dan top_n

    # Membangun model Collaborative Filtering berdasarkan artikel relevan
    distances, indices = item_based_collaborative_filtering(relevant_data, top_n)

    # Menyusun rekomendasi artikel dari hasil Collaborative Filtering
    recommended_articles_collaborative = relevant_data.iloc[indices[0]].copy()
    recommended_articles_collaborative['Similarity'] = 1 - distances[0]

    # Menambahkan Artikel ID pada rekomendasi artikel
    recommended_articles_collaborative['Artikel ID'] = recommended_articles_collaborative.index

    # Menyusun hasil rekomendasi
    recommended_articles_collaborative = recommended_articles_collaborative[['Artikel ID', 'Authors', 'Title', 'Cites', 'Type', 'DOI', 'Similarity']]
    recommended_articles_collaborative = recommended_articles_collaborative.sort_values(by='Similarity', ascending=False)

    return recommended_articles_collaborative


# Contoh penggunaan sistem rekomendasi
print("keyword untuk mencari artikel: ",user_keyword)
recommended_articles_collaborative = recommend_articles(user_keyword, data_cleaned, top_n=10)

# Menampilkan hasil rekomendasi dalam bentuk tabel menggunakan DataTable
from google.colab import data_table
data_table.DataTable(recommended_articles_collaborative)

"""**Precision@K**

Precision@K digunakan untuk mengevaluasi kualitas rekomendasi artikel.
Fungsi `evaluate_recommendations` membandingkan **K artikel teratas** yang direkomendasikan dengan artikel yang relevan (ditentukan oleh sitasi > 10). Precision@K dihitung sebagai proporsi artikel relevan di antara **K rekomendasi teratas**. Hasilnya menunjukkan seberapa baik sistem dalam memberikan rekomendasi yang relevan. Outputnya adalah nilai **Precision@K**, yang menggambarkan akurasi sistem dalam merekomendasikan artikel relevan.
"""

# Evaluasi Precision@K
def evaluate_recommendations(recommended_articles, relevant_articles, k=10):
    """
    recommended_articles: DataFrame hasil rekomendasi
    relevant_articles: DataFrame yang berisi artikel relevan yang sesungguhnya
    k: jumlah rekomendasi teratas yang akan dievaluasi
    """

    # Ambil K artikel teratas
    top_k_recommendations = recommended_articles.head(k)

    # Ambil artikel relevan yang ada dalam daftar artikel yang relevan
    relevant_articles_set = set(relevant_articles['Title'])

    # Hitung Precision@K
    relevant_in_top_k = top_k_recommendations[top_k_recommendations['Title'].isin(relevant_articles_set)]
    precision_at_k = len(relevant_in_top_k) / k

    return precision_at_k

# Misalkan 'relevant_articles' adalah daftar artikel yang relevan berdasarkan sitasi
relevant_articles = data_cleaned[data_cleaned['Cites'] > 10]  # Artikel dengan sitasi > 10 dianggap relevan

# Melakukan evaluasi untuk artikel yang direkomendasikan
precision_at_k = evaluate_recommendations(recommended_articles_collaborative, relevant_articles, k=10)

# Menampilkan hasil evaluasi
print(f"Precision@10: {precision_at_k:.4f}")

"""# **MEMBANDINGKAN DAN MENGURUTKAN HASIL REKOMENDASI**

Fungsi yang dibuat berfungsi untuk membandingkan hasil rekomendasi dari dua metode yang berbeda, yaitu *Content-Based Filtering* dan *Hybrid Method*. Pertama, kode ini memeriksa apakah kedua DataFrame yang berisi hasil rekomendasi tersebut kosong. Jika tidak, kolom baru yang disebut 'Method' akan ditambahkan untuk menandai sumber masing-masing rekomendasi, di mana satu berasal dari *Content-Based* dan yang lainnya dari metode *Hybrid-Based (Collaborative Filtering)*. Kedua DataFrame ini kemudian digabungkan, dan hasilnya diurutkan berdasarkan jumlah sitasi, dari yang terbanyak. Setelah pengurutan, hasil rekomendasi ditampilkan menggunakan Google Colab’s `data_table.DataTable`, yang memungkinkan data untuk dilihat dalam format tabel yang lebih terstruktur dan mudah dipahami. Dengan demikian, fungsi ini memberikan cara yang efisien untuk membandingkan dan menilai hasil rekomendasi berdasarkan sitasi artikel.
"""

import pandas as pd
from google.colab import data_table

def compare_and_sort_results_by_cites(content_based_df, collaborative_df):
    # Cek apakah DataFrame valid
    if content_based_df.empty or collaborative_df.empty:
        print("Salah satu DataFrame kosong!")
        return

    # Menambahkan kolom 'Method' untuk membedakan antara Content-Based dan Collaborative Filtering
    content_based_df['Method'] = 'Content-Based'
    collaborative_df['Method'] = 'Hybrid-Based'

    # Menggabungkan kedua DataFrame
    all_recommendations = pd.concat([content_based_df, collaborative_df])

    # Mengurutkan berdasarkan kolom 'Cites' dari yang terbanyak
    all_recommendations_sorted = all_recommendations.sort_values(by='Cites', ascending=False)

    # Menampilkan hasil menggunakan display() untuk memastikan tampilannya
    from IPython.display import display
    display(all_recommendations_sorted)  # Menampilkan DataFrame dengan display()
    data_table.enable_dataframe_formatter()
    data_table.DataTable(all_recommendations_sorted)

# Contoh Penggunaan
print("keyword untuk mencari artikel: ",user_keyword)
compare_and_sort_results_by_cites(recommended_articles_content, recommended_articles_collaborative)